name: Backup Database to Azure Storage

on:
  workflow_dispatch:
    inputs:
      overwriteThisMorningsBackup:
        required: true
        type: boolean
        default: false
      restoreToProductionDataEnv:
        required: true
        type: boolean
        default: false
  schedule: # 03:00 UTC
    - cron: '0 3 * * *'

jobs:
  backup:
    name: Backup PaaS Database (production)
    if: ${{ github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.overwriteThisMorningsBackup == 'true') }}
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:11.10
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: postgres
        ports:
        - 5432:5432
        options: --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5
    steps:
    - uses: actions/checkout@v3
      name: Checkout

    - name: Setup cf cli
      uses: DFE-Digital/github-actions/setup-cf-cli@master
      with:
        CF_USERNAME:   ${{ secrets.CF_USERNAME_PRODUCTION }}
        CF_PASSWORD:   ${{ secrets.CF_PASSWORD_PRODUCTION }}
        CF_SPACE_NAME: ${{ secrets.CF_SPACE_PRODUCTION }}
        INSTALL_CONDUIT: true

    - name: Setup postgres client
      uses: DFE-Digital/github-actions/install-postgres-client@master

    - name: Set environment variable
      run: echo "BACKUP_FILE_NAME=register_prod_$(date +"%F")" >> $GITHUB_ENV

    - name: Backup Prod DB
      run: |
        cf conduit register-postgres-production -- pg_dump -E utf8 --clean --if-exists --no-owner --verbose --no-password -f ${BACKUP_FILE_NAME}.sql
        tar -cvzf ${BACKUP_FILE_NAME}.tar.gz ${BACKUP_FILE_NAME}.sql

    - name: Upload Backup to Azure Storage
      run: |
        az storage blob upload --container-name prod-db-backup \
        --file ${BACKUP_FILE_NAME}.tar.gz --name ${BACKUP_FILE_NAME}.tar.gz --overwrite \
        --connection-string '${{ secrets.AZURE_PROD_DB_BACKUP_STORAGE_CONNECTION_STRING }}'

    - name: Sanitise the Database backup
      run: |
        echo "::group::Restore backup to intermediate database"
        createdb ${DATABASE_NAME} && psql -f ${{ env.BACKUP_FILE_NAME }}.sql -d ${DATABASE_NAME}
        echo "::endgroup::"

        echo "::group::Sanitise user data"
        psql -d ${DATABASE_NAME} -f db/scripts/sanitise.sql
        echo "::endgroup::"

        echo "::group::Backup Sanitised Database"
        pg_dump --encoding utf8 --clean --no-owner --if-exists -d ${DATABASE_NAME} -f backup_sanitised.sql
        echo "::endgroup::"
      env:
        DATABASE_NAME: register_trainee_teachers
        PGUSER:  postgres
        PGPASSWORD: postgres
        PGHOST: localhost
        PGPORT: 5432

    - name: Upload Sanitised Backup
      uses: actions/upload-artifact@v3
      with:
        name: backup_sanitised
        path: backup_sanitised.sql
        retention-days: 3

    - name: Restore backup to productiondata
      if: ${{ github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.restoreToProductionDataEnv == 'true') }}
      run: |
        cf conduit register-postgres-productiondata -- psql < ${{ env.BACKUP_FILE_NAME }}.sql
        rm ${BACKUP_FILE_NAME}.sql
